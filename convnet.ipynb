{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We will start by taking a look at a simple convnet example that classifies the MNIST digits.\n",
        "The following shows an example of a basic Convnet; a stack of Conv2D and MaxPooling2D layers.\n",
        "And as we mostly do, we will use the functional API to build the model:"
      ],
      "metadata": {
        "id": "rQ7Dm9X1K1Tc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CJ48KNIV9jPc"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(28, 28, 1))    # (shape=(image_height, image_width, image_channels)), not including the batch dim.\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs) #filters=32 means the layer will learn 32 feature detectors like edges, shapes etc.\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)   # kernel size is the size of the filter window, 3 * 3\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)                            # pooling reduces the spatial size for better learning\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "snO6gt38L1zm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convnet takes input tensors of shape(image_height, image_width, image_channel) without including the batch dimension. Here, we wukk configure the convnet to process inputs of size (28, 28, 1) — the format of the MNIST images. with 1 representing grayscale."
      ],
      "metadata": {
        "id": "ikmNqjR9YSfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets display the architecture of our convent."
      ],
      "metadata": {
        "id": "mktR3pvNaIyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "_mH_7nc9YRgV",
        "outputId": "be51649b-3de1-4767-9e1f-c5e70284be70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m11,530\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,530</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,202\u001b[0m (407.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,202</span> (407.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,202\u001b[0m (407.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,202</span> (407.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see each output of the Conv2D and Maxpooling layer is a rank-3 tensor, with the filter argument passed to the Conv2D layer controlling the number of channels.\n",
        "\n",
        "After the last Conv2D layer, we ended up with (3, 3, 128) output shape. that is a 3 by 3 feature map with 128 channels. Then we feed this output layer into a densely connected classifer that processes 1D vectors. So for them to be compatible, we flatten them out to 1D before adding the dense layer."
      ],
      "metadata": {
        "id": "jdnW4odLfj7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets train our convnet using the mnist dataset. we will use the sparse_categorical_crossentropy because our labels are integers"
      ],
      "metadata": {
        "id": "FVUJ6b7MibK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "ADIfHAfWaXrn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eidZQAXCjEtX",
        "outputId": "c62ba58d-d059-4454-e46f-da8e07b933a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 94ms/step - accuracy: 0.8860 - loss: 0.3680\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 94ms/step - accuracy: 0.9843 - loss: 0.0510\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 89ms/step - accuracy: 0.9903 - loss: 0.0296\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 93ms/step - accuracy: 0.9931 - loss: 0.0224\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 89ms/step - accuracy: 0.9956 - loss: 0.0164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7da1211b68d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpW4lIDMmYGq",
        "outputId": "a582990a-4052-4d32-d70e-ad64cd5b8f76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9892 - loss: 0.0353\n",
            "Test accuracy: 0.992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we have an accuracy as high as 99.2%. This works better than the densely connnected model explored in earlier chapters. This is because of features like filters and Maxpooling — more details in the book."
      ],
      "metadata": {
        "id": "gui52FwLpodh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training convnets from scratch on a small dataset\n",
        "\n",
        "we will classify images as dogs and cats in a dataset containing 5000 pictures of cats and dogs(2500 cats, and 2500 pics of dogs).\n",
        "\n",
        "We will first naively train 2000 images from scratch without regularization, to set a baseline for what can be achieved. Before then exploring data augmentation to imporve the model.\n",
        "\n",
        "In the next section, we will explore *feature extraction with a pretrained model* and *fine tuning a pretrained model*, all of which will improve our model immensely."
      ],
      "metadata": {
        "id": "a9OFZcmPp2nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets download the data set from kaggle. But doing that, I will need to authenticate myself on kaggel using the kaggle token. Lets do it :"
      ],
      "metadata": {
        "id": "R6cUkYSOr64-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "RxrEgYppj_Um"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = {\n",
        "     'username': 'mainasaid93',\n",
        "     'key': 'KGAT_d2c2edae0d4e484013aec1c00e95764c'\n",
        " }\n",
        "\n",
        "with open(\"kaggle.json\", \"w\") as t:         # creates a json file called kaggle.json, 'w' write mode\n",
        "  json.dump(token, t)                # dumps the token (the dictionary above) into the file t. in a proper json format.\n",
        "\n",
        "!mkdir ~/.kaggle              # creating a kaggle folder\n",
        "!cp kaggle.json ~/.kaggle/   # coppying the key file to it.\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # making it only readable by the user, that is myself in this case."
      ],
      "metadata": {
        "id": "L853oCULkAvP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCnGHirnouR0",
        "outputId": "9ccbe978-788f-47be-c8cd-b41681f5bed7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                               title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------------  --------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "wardabilal/spotify-global-music-dataset-20092025                  Spotify Global Music Dataset (2009–2025)               1289021  2025-11-11 09:43:05.933000           9822        224  1.0              \n",
            "sadiajavedd/students-academic-performance-dataset                 Students_Academic_Performance_Dataset                     8907  2025-10-23 04:16:35.563000          14716        348  1.0              \n",
            "khushikyad001/ai-impact-on-jobs-2030                              AI Impact on Jobs 2030                                   87410  2025-11-09 17:58:05.410000           5634        129  1.0              \n",
            "sonalshinde123/food-nutrition-dataset-150-everyday-foods          Food Nutrition Dataset                                    5566  2025-11-15 11:19:11.410000           1566         29  1.0              \n",
            "umitka/food-price-inflation                                       Food Price Inflation                                    473026  2025-12-02 12:30:45.013000            799         26  1.0              \n",
            "rohiteng/amazon-sales-dataset                                     Amazon Sales Dataset                                   4037578  2025-11-23 14:29:37.973000           2695         36  1.0              \n",
            "prince7489/youtube-shorts-performance-dataset                     YouTube Shorts Performance Dataset                        6541  2025-11-25 09:23:36.147000           1096         33  0.9411765        \n",
            "kundanbedmutha/instagram-analytics-dataset                        Instagram Analytics Dataset                            1090208  2025-11-19 09:28:48.650000           2586         49  1.0              \n",
            "umitka/global-youth-unemployment-dataset                          Global Youth Unemployment Dataset                        83620  2025-11-29 10:14:28.613000           1071         42  1.0              \n",
            "yogape/logistics-operations-database                              Logistics Operations Database                         15059576  2025-11-23 20:21:26.907000            929         24  1.0              \n",
            "sonalshinde123/student-academic-performance-dataset               Student Academic Performance Dataset                     14999  2025-11-27 12:19:48.120000           1472         39  1.0              \n",
            "ayeshaseherr/buisness-sales                                       Buisness_Sales                                          954452  2025-11-24 08:51:12.203000           1050         31  1.0              \n",
            "kundanbedmutha/exam-score-prediction-dataset                      Exam Score Prediction Dataset                           325454  2025-11-28 07:29:01.047000           1614         32  1.0              \n",
            "kundanbedmutha/healthcare-symptomsdisease-classification-dataset  Healthcare Symptoms–Disease Classification Dataset      373982  2025-11-16 12:51:52.617000           1222         32  1.0              \n",
            "sonalshinde123/social-media-mental-health-indicators-dataset      Social Media Mental Health Indicators Dataset            80135  2025-11-26 09:23:50.673000           1663         38  1.0              \n",
            "shaistashahid/job-market-insight                                  Job Market Insight                                        7558  2025-11-27 09:24:45.490000            615         28  1.0              \n",
            "wardabilal/student-stress-analysis                                Student Stress Analysis                                   1729  2025-11-01 09:14:39.367000           5544         98  1.0              \n",
            "prince7489/fast-food-ordering-pattern-dataset                     Fast-Food Ordering Pattern Dataset                        9413  2025-11-23 13:42:34.117000            968         32  0.9411765        \n",
            "umitka/synthetic-financial-fraud-dataset                          Financial Fraud Dataset                                 375568  2025-11-25 12:01:17.420000           1153         41  1.0              \n",
            "zubairdhuddi/student-dataset                                      Student Mental Health & Depression Indicators Data      467020  2025-11-17 14:30:05.707000           1586         33  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows everything has worked. Let me now download the dataset needed for this model."
      ],
      "metadata": {
        "id": "NdUfxTNWpnAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5999ItNnpkRD",
        "outputId": "36bfdd63-ba51-4a8f-9dd4-d82edf2f9945"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/dogs-vs-cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The competition has officically ended so I cannot join, that is why the above code will not work. To work with the dataset for practice like i am doing, just download the datasets — done by only changing *competitions* with *datasets* in the code."
      ],
      "metadata": {
        "id": "sJtp43xqvDwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d tongpython/cat-and-dog"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1i1zrALsxnA",
        "outputId": "f3666dd2-f5ce-4a85-978e-fd7fefed5281"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tongpython/cat-and-dog\n",
            "License(s): CC0-1.0\n",
            "Downloading cat-and-dog.zip to /content\n",
            " 89% 194M/218M [00:00<00:00, 617MB/s]  \n",
            "100% 218M/218M [00:00<00:00, 632MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip -qq cat-and-dog.zip -d cat-and-dog  # unzips it to a folder named cat-and-dog"
      ],
      "metadata": {
        "id": "RoxMQYHDuy1y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now instead of downloading the data anytime i want to continue with it on colab, I will download it on my system and upload it on google drive. this way i have it readily available for use. lets do it."
      ],
      "metadata": {
        "id": "cl6IvzMOyGy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q8Z6S_KjOeY",
        "outputId": "db2f2132-a3e3-4a74-e90d-a5929da1b6df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib, zipfile"
      ],
      "metadata": {
        "id": "0KOZkyK0ARo_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to make my folder structure same with the one in the book.by doing so, I will also practice how this libararies are used. Remember i could have done it while creating the zip file, I am doing more (not necessary) to practice working with the os, pathlib, and shutil libarary.\n",
        "\n",
        "### creating the book style\n"
      ],
      "metadata": {
        "id": "w8v1-ASMxApU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "old_path = \"/content/drive/MyDrive/cats_vs_dogs_small\"\n",
        "new_path = \"/content/drive/MyDrive/cats_vs_dogs_original\"\n",
        "\n",
        "os.rename(old_path, new_path)\n",
        "\n",
        "print(\"Folder renamed successfully!\")\n",
        "print(new_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyWOYTEq3Hwg",
        "outputId": "6d690c8e-43bd-49ec-902e-7f194d028530"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder renamed successfully!\n",
            "/content/drive/MyDrive/cats_vs_dogs_original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_small = \"/content/drive/MyDrive/cats_vs_dogs_small\"\n",
        "os.makedirs(new_small, exist_ok=True)"
      ],
      "metadata": {
        "id": "KOx_mgGWqVqS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I have it like it is in the book. so we continue with the codes in the book.\n",
        "\n",
        "Recap: I first downloaded the the raw data, extracted it and move it into the the folder. i did some more steps to work with the pathlib and os just for practice."
      ],
      "metadata": {
        "id": "CDxQjkWlxhS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/drive/MyDrive/train\", exist_ok=True)"
      ],
      "metadata": {
        "id": "7zn99Ua53FbU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets copy the data into the new train folder:"
      ],
      "metadata": {
        "id": "T0OfSMCe4Cv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cats_src = \"/content/drive/MyDrive/cats_vs_dogs_original/training_set/training_set/cats\"\n",
        "# dogs_src = \"/content/drive/MyDrive/cats_vs_dogs_original/training_set/training_set/dogs\"\n",
        "\n",
        "# train_dir = \"/content/drive/MyDrive/train\"\n",
        "\n",
        "# # Copy cat images\n",
        "# for i, fname in enumerate(sorted(os.listdir(cats_src))):\n",
        "#     if fname.lower().endswith(\".jpg\"):\n",
        "#         shutil.copyfile(\n",
        "#             os.path.join(cats_src, fname),\n",
        "#             os.path.join(train_dir, f\"cat.{i}.jpg\")\n",
        "#         )\n",
        "\n",
        "# # Copy dog images\n",
        "# for i, fname in enumerate(sorted(os.listdir(dogs_src))):\n",
        "#     if fname.lower().endswith(\".jpg\"):\n",
        "#         shutil.copyfile(\n",
        "#             os.path.join(dogs_src, fname),\n",
        "#             os.path.join(train_dir, f\"dog.{i}.jpg\")\n",
        "#         )"
      ],
      "metadata": {
        "id": "3myVhqc0adrD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#original_directory = pathlib.Path(\"cat-and-dog/training_set/training_set\") # path to the direcotry where our origin dataset was stored\n",
        "#new_base_dir = pathlib.Path(\"cats_and_dogs_small\")  # directory where our small dataset will be stored."
      ],
      "metadata": {
        "id": "3mR0S9ff-Gpy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(\"/content/drive/MyDrive/train\")\n",
        "print(len(files))"
      ],
      "metadata": {
        "id": "cW1oRfoUBDpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23a9f7b-dcb5-4204-dee1-fab23e1b61f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets me inspect the content of our dataset."
      ],
      "metadata": {
        "id": "V_r7DgkQVlhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(files[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mB7-d8yOORT",
        "outputId": "73f08262-2801-46a6-fa37-1bb6760ad802"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dog.3006.jpg', 'dog.3007.jpg', 'dog.3008.jpg', 'dog.3009.jpg', 'dog.3010.jpg', 'dog.3011.jpg', 'dog.3012.jpg', 'dog.3013.jpg', 'dog.3014.jpg', 'dog.3015.jpg', 'dog.3016.jpg', 'dog.3017.jpg', 'dog.3018.jpg', 'dog.3019.jpg', 'dog.3020.jpg', 'dog.3021.jpg', 'dog.3022.jpg', 'dog.3023.jpg', 'dog.3024.jpg', 'dog.3025.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(files[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfFsiZLORAn",
        "outputId": "481a2e89-4320-47cc-e98a-b2635e89cd6d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat.991.jpg', 'cat.992.jpg', 'cat.993.jpg', 'cat.994.jpg', 'cat.995.jpg', 'cat.996.jpg', 'cat.997.jpg', 'cat.998.jpg', 'cat.999.jpg', 'cat.1000.jpg', 'cat.1001.jpg', 'cat.1002.jpg', 'cat.1003.jpg', 'cat.1004.jpg', 'cat.1005.jpg', 'cat.1.jpg', 'cat.2.jpg', 'cat.3.jpg', 'cat.4.jpg', 'cat.5.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cats_count = 0\n",
        "dogs_counts = 0\n",
        "for i in files:\n",
        "  if i.startswith(\"cat\"):\n",
        "    cats_count += 1\n",
        "  elif i.startswith(\"dog\"):\n",
        "    dogs_counts += 1\n",
        "\n",
        "print(f\"cats: {cats_count}\")\n",
        "print(f\"dogs: {dogs_counts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN1nE30uRtZn",
        "outputId": "574a83d5-f3aa-4374-bd01-3e90ed4bf946"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats: 4000\n",
            "dogs: 4005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or a more cleaner/pythonic approach:"
      ],
      "metadata": {
        "id": "R6DYFF87c6tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cats_count = [cats for cats in files if cats.startswith(\"cat\")]\n",
        "dogs_counts = [dogs for dogs in files if dogs.startswith(\"dog\")]\n",
        "\n",
        "print(f\"length of cat pictures: {len(cats_count)}\")\n",
        "print(f\"length of dog pictures: {len(dogs_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjvOi1Q1bii9",
        "outputId": "6d664331-37d2-4731-8ba7-770686e0ff16"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of cat pictures: 4000\n",
            "length of dog pictures: 4005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have 4000 cat pictures and 4005 dog pictures. that is almost split in halve. remember we are not going to use all of the images. Just 5000 for training, validation, and testing out of the 8005 total we have."
      ],
      "metadata": {
        "id": "6w0uXy5-XuJS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E91hoUTLWtU6"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}