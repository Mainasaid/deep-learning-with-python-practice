{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lKHQZVSQSdBf"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():                      # for the model to be reusable later\n",
        "  inputs = keras.Input((28 * 28,))\n",
        "  features = layers.Dense(512, activation='relu')(inputs)\n",
        "  features = layers.Dropout(0.5)(features)                 # randomly reducing the neurons to reduce overfitting.\n",
        "  outputs = layers.Dense(10, activation='softmax')(features)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yZgHc62Pceez"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the mnist data\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "id": "sYa6bXK4cmEt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When writing a training loop, remember to pass \"training=True\" during the forward pass so that it behaves in a training mode (and Not inference mode)\n",
        "\n",
        "Also, to retrieve the gradients of the weights of the model, \"model.trainable_weights\" should be used, so that the weights will be updated via backpropagation to minize the loss of the model, such as in kernel, 'W' and bias, 'b'."
      ],
      "metadata": {
        "id": "jyWhP1ZDLjFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low level usage of metrics\n",
        "\n",
        "in the low-level training loop, you'll likely want to leverage the keras metrics.\n",
        "\n",
        "for the API: simply call update_state(y_true, y_pred) for each batch and predictions, and then use the result() argument to querry the current metric value. Lets see:"
      ],
      "metadata": {
        "id": "MrQeIMIBRX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result : {current_result:.2f}\")"
      ],
      "metadata": {
        "id": "Q1xBjSP0O2VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6c89c9-f680-44ec-c3e6-3c8f1ce46df0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, to track the average of a scaler like the model's loss, you can use the keras.mmetrics.Mean() metric as follows"
      ],
      "metadata": {
        "id": "_SS11zzkTr-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for val in values:\n",
        "  mean_tracker.update_state(val)\n",
        "print(f\"Mean of values : {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN-jvGpRTpsk",
        "outputId": "a9dc6ea0-bea9-4c3c-fbfa-c0b138558935"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values : 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Remember to use metric.reset_state() when you want to reset the current results (at the start of a training epoch or at the start of evaluation). Just as the one used in the custom metric: \"RootMeanSquaredError()\" (check Testing.ipynb file for reference)"
      ],
      "metadata": {
        "id": "pEMg_HjAWPye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with all that we've seen, Lets now look at a complete training and evaluation loop.\n",
        "## Complete training and evaluation loop"
      ],
      "metadata": {
        "id": "kMqoLCLzW8df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the training loop function\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # prepares the list of metric to monitor. reports on how well is the model doin\n",
        "loss_tracking_metric = keras.metrics.Mean()     # keeps track of avrage of the losses\n",
        "\n",
        "def train_step(inputs, targets):                        # define one training iteration(one batch) using a custom loop instead of model.fit()\n",
        "  with tensorflow.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)   # update the metrics's internal state/accumulators.\n",
        "    logs[metric.name] = metric.result()         # assigns the metric current value to the metric (name)\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "SMbTgQQrU4YS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_K0cXb_QDse"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}