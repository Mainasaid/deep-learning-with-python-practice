{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lKHQZVSQSdBf"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():                      # for the model to be reusable later\n",
        "  inputs = keras.Input((28 * 28,))\n",
        "  features = layers.Dense(512, activation='relu')(inputs)\n",
        "  features = layers.Dropout(0.5)(features)                 # randomly reducing the neurons to reduce overfitting.\n",
        "  outputs = layers.Dense(10, activation='softmax')(features)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yZgHc62Pceez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the mnist data\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYa6bXK4cmEt",
        "outputId": "24c4b4ca-03ba-43df-82bd-3a09e45bc10b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When writing a training loop, remember to pass \"training=True\" during the forward pass so that it behaves in a training mode (and Not inference mode)\n",
        "\n",
        "Also, to retrieve the gradients of the weights of the model, \"model.trainable_weights\" should be used, so that the weights will be updated via backpropagation to minize the loss of the model, such as in kernel, 'W' and bias, 'b'."
      ],
      "metadata": {
        "id": "jyWhP1ZDLjFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low level usage of metrics\n",
        "\n",
        "in the low-level training loop, you'll likely want to leverage the keras metrics.\n",
        "\n",
        "for the API: simply call update_state(y_true, y_pred) for each batch and predictions, and then use the result() argument to querry the current metric value. Lets see:"
      ],
      "metadata": {
        "id": "MrQeIMIBRX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result : {current_result:.2f}\")"
      ],
      "metadata": {
        "id": "Q1xBjSP0O2VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efdc5f9-833f-49eb-b496-dce30014adc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, to track the average of a scaler like the model's loss, you can use the keras.mmetrics.Mean() metric as follows"
      ],
      "metadata": {
        "id": "_SS11zzkTr-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for val in values:\n",
        "  mean_tracker.update_state(val)\n",
        "print(f\"Mean of values : {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN-jvGpRTpsk",
        "outputId": "14ef4d0f-e0b5-477e-a385-5c48a7b192e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values : 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Remember to use metric.reset_state() when you want to reset the current results (at the start of a training epoch or at the start of evaluation). Just as the one used in the custom metric: \"RootMeanSquaredError()\" (check Testing.ipynb file for reference)"
      ],
      "metadata": {
        "id": "pEMg_HjAWPye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with all that we've seen, Lets now look at a complete training and evaluation loop.\n",
        "## Complete training and evaluation loop"
      ],
      "metadata": {
        "id": "kMqoLCLzW8df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the training loop function\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # prepares the list of metric to monitor. reports on how well is the model doin\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()     # keeps track of avrage of the losses\n",
        "\n"
      ],
      "metadata": {
        "id": "SMbTgQQrU4YS"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}