{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lKHQZVSQSdBf"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():                      # for the model to be reusable later\n",
        "  inputs = keras.Input((28 * 28,))\n",
        "  features = layers.Dense(512, activation='relu')(inputs)\n",
        "  features = layers.Dropout(0.5)(features)                 # randomly reducing the neurons to reduce overfitting.\n",
        "  outputs = layers.Dense(10, activation='softmax')(features)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yZgHc62Pceez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the mnist data\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "id": "sYa6bXK4cmEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1484fac6-a4ae-4662-cccf-923541606465"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When writing a training loop, remember to pass \"training=True\" during the forward pass so that it behaves in a training mode (and Not inference mode)\n",
        "\n",
        "Also, to retrieve the gradients of the weights of the model, \"model.trainable_weights\" should be used, so that the weights will be updated via backpropagation to minize the loss of the model, such as in kernel, 'W' and bias, 'b'."
      ],
      "metadata": {
        "id": "jyWhP1ZDLjFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low level usage of metrics\n",
        "\n",
        "in the low-level training loop, you'll likely want to leverage the keras metrics.\n",
        "\n",
        "for the API: simply call update_state(y_true, y_pred) for each batch and predictions, and then use the result() argument to querry the current metric value. Lets see:"
      ],
      "metadata": {
        "id": "MrQeIMIBRX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result : {current_result:.2f}\")"
      ],
      "metadata": {
        "id": "Q1xBjSP0O2VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264306ae-d9e1-4e1f-8fc7-0eab18ed6790"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, to track the average of a scaler like the model's loss, you can use the keras.mmetrics.Mean() metric as follows"
      ],
      "metadata": {
        "id": "_SS11zzkTr-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for val in values:\n",
        "  mean_tracker.update_state(val)\n",
        "print(f\"Mean of values : {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN-jvGpRTpsk",
        "outputId": "114bf4ee-ba41-469b-f357-032bb07f3fe7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values : 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Remember to use metric.reset_state() when you want to reset the current results (at the start of a training epoch or at the start of evaluation). Just as the one used in the custom metric: \"RootMeanSquaredError()\" (check Testing.ipynb file for reference)"
      ],
      "metadata": {
        "id": "pEMg_HjAWPye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with all that we've seen, Lets now look at a complete training and evaluation loop.\n",
        "## Complete training and evaluation loop"
      ],
      "metadata": {
        "id": "kMqoLCLzW8df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the training loop function\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # prepares the list of metric to monitor. reports on how well is the model doin\n",
        "loss_tracking_metric = keras.metrics.Mean()     # keeps track of avrage of the losses\n",
        "\n",
        "def train_step(inputs, targets):                        # define one training iteration(one batch) using a custom loop instead of model.fit()\n",
        "  with tensorflow.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)      # run the forward pass. training=True means it behaves in a training mode as said earlier\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)   # update the metrics's internal state/accumulators.\n",
        "    logs[metric.name] = metric.result()         # assigns the metric current value to the metric (name)\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "SMbTgQQrU4YS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to reset our metrics at the begining of each epoch and before the running evaluation. Thus ensures each epoch's metric value reflects the performance of only the epoch. lets see it below"
      ],
      "metadata": {
        "id": "A0MkNRJjRbj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "C_K0cXb_QDse"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets layout our complete training loop. notice we use tf.data.Dataset object, this turns our tensor data (a Numpy data) into iterator. It splits our data into individual (x,y) pairs that tensorflow can efficiently feed into our model. Lets write the complete training loop:"
      ],
      "metadata": {
        "id": "mRTcKDMN3DcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tensorflow.data.Dataset.from_tensor_slices((\n",
        "    train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)  # returned from the train_step function above. that is the metric, loss and their respective values\n",
        "  print(f\"results at the end of epoch {epoch}\")\n",
        "  for key, values in logs.items():\n",
        "    print(f\"... {key} : {values: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwnhVDC93APT",
        "outputId": "99256fa0-49e3-4b1f-9feb-87ca0777e1d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results at the end of epoch 0\n",
            "... sparse_categorical_accuracy :  0.9140\n",
            "... loss :  0.2915\n",
            "results at the end of epoch 1\n",
            "... sparse_categorical_accuracy :  0.9545\n",
            "... loss :  0.1579\n",
            "results at the end of epoch 2\n",
            "... sparse_categorical_accuracy :  0.9642\n",
            "... loss :  0.1311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is a training loop. we can also see that our model is learning well — with the loss value going down and the metric going up."
      ],
      "metadata": {
        "id": "H1_nJCgqAPMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets look at the evaluation loop. its a simple for-loop that repeatedly calls the test_step() function, which processes a single batch of data. This functions is just a subset of the logic of train_step() function. That is to say it does not have codes that deals with updating the weights as in training : everything that involves Gradient_tape() and optimizer. lets see"
      ],
      "metadata": {
        "id": "H-BMvDioZ4Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(inputs, targets):\n",
        "  predictions = model(inputs, training=False)    # training=False here because it is for inference. not training, so no updating weights as you can see\n",
        "  loss = loss_fn(targets, predictions)\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)    # updates the metric internal state accumulator\n",
        "    logs[metric.name] = metric.result()          # assigns the metric current value to the metric\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)        # updates the average metric loss\n",
        "  logs['v_loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "yBboKIJWAisC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tensorflow.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs = test_step(inputs_batch, targets_batch)\n",
        "print('evaluation results:')\n",
        "for key, value in logs.items():\n",
        "  print(f'...{key}: {value:.4f}')"
      ],
      "metadata": {
        "id": "zZxZinNlIf4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deab82fd-7dd5-431a-848d-b87ea5625770"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation results:\n",
            "...sparse_categorical_accuracy: 0.9650\n",
            "...v_loss: 0.1288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have just implemented fit() and evaluation() method."
      ],
      "metadata": {
        "id": "XyG2xcGcgBUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "one noticeable thing is that these above custom loops, especially for fit runs significantly slower than the in-built fit(), and evaluation() method. we will take a look at tf.function optimizer next — to make it faster."
      ],
      "metadata": {
        "id": "TAkCJEK7gKBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the custom loops fast with tf.function\n",
        "The syntax to do this is very easy, just add @tf.function at the any function you want to compile before running it."
      ],
      "metadata": {
        "id": "dpd_RZQv32uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tNWZclNBfvIa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function                                      # Note that this is the only line that changed\n",
        "def train_step(inputs, targets):\n",
        "  with tensorflow.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[metric.name] = metric.result()\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs\n",
        "\n",
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "SegcJzDpCOtn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will run the training loop and compare the timing of execution with the first one without the @tf.function."
      ],
      "metadata": {
        "id": "73t5blY9Dq49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tensorflow.data.Dataset.from_tensor_slices((\n",
        "    train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)\n",
        "  print(f\"results at the end of epoch {epoch}\")\n",
        "  for key, values in logs.items():\n",
        "    print(f\"... {key} : {values: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egi8xfJIDX6B",
        "outputId": "804cd1c8-3a64-4545-9c24-6cc8260e3c7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results at the end of epoch 0\n",
            "... sparse_categorical_accuracy :  0.9679\n",
            "... loss :  0.1148\n",
            "results at the end of epoch 1\n",
            "... sparse_categorical_accuracy :  0.9722\n",
            "... loss :  0.1044\n",
            "results at the end of epoch 2\n",
            "... sparse_categorical_accuracy :  0.9742\n",
            "... loss :  0.0938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And yes, we just went from 2 minutes execution time to just 27 seconds using @tf.function on the train_step() function on google colab CPU."
      ],
      "metadata": {
        "id": "1VjqxGWnFkIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Remember that while debugging your code, its better you run it eagerly, i.e step by step, without the @tf.function.  once your code is okay, then you can add the @tf.function decorator to make it fast."
      ],
      "metadata": {
        "id": "2HoN0F_YOmKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qncXP4WSOlNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}