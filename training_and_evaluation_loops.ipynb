{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lKHQZVSQSdBf"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():                      # for the model to be reusable later\n",
        "  inputs = keras.Input((28 * 28,))\n",
        "  features = layers.Dense(512, activation='relu')(inputs)\n",
        "  features = layers.Dropout(0.5)(features)                 # randomly reducing the neurons to reduce overfitting.\n",
        "  outputs = layers.Dense(10, activation='softmax')(features)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yZgHc62Pceez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the mnist data\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "id": "sYa6bXK4cmEt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When writing a training loop, remember to pass \"training=True\" during the forward pass so that it behaves in a training mode (and Not inference mode)\n",
        "\n",
        "Also, to retrieve the gradients of the weights of the model, \"model.trainable_weights\" should be used, so that the weights will be updated via backpropagation to minize the loss of the model, such as in kernel, 'W' and bias, 'b'."
      ],
      "metadata": {
        "id": "jyWhP1ZDLjFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low level usage of metrics\n",
        "\n",
        "in the low-level training loop, you'll likely want to leverage the keras metrics.\n",
        "\n",
        "for the API: simply call update_state(y_true, y_pred) for each batch and predictions, and then use the result() argument to querry the current metric value. Lets see:"
      ],
      "metadata": {
        "id": "MrQeIMIBRX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result : {current_result:.2f}\")"
      ],
      "metadata": {
        "id": "Q1xBjSP0O2VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c0cfcf-4819-44d1-eb46-6aef65b4f73f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, to track the average of a scaler like the model's loss, you can use the keras.mmetrics.Mean() metric as follows"
      ],
      "metadata": {
        "id": "_SS11zzkTr-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for val in values:\n",
        "  mean_tracker.update_state(val)\n",
        "print(f\"Mean of values : {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN-jvGpRTpsk",
        "outputId": "ba598dc3-8d23-4e2a-c039-8d14782c6299"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values : 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Remember to use metric.reset_state() when you want to reset the current results (at the start of a training epoch or at the start of evaluation). Just as the one used in the custom metric: \"RootMeanSquaredError()\" (check Testing.ipynb file for reference)"
      ],
      "metadata": {
        "id": "pEMg_HjAWPye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with all that we've seen, Lets now look at a complete training and evaluation loop.\n",
        "## Complete training and evaluation loop"
      ],
      "metadata": {
        "id": "kMqoLCLzW8df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the training loop function\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # prepares the list of metric to monitor. reports on how well is the model doin\n",
        "loss_tracking_metric = keras.metrics.Mean()     # keeps track of avrage of the losses\n",
        "\n",
        "def train_step(inputs, targets):                        # define one training iteration(one batch) using a custom loop instead of model.fit()\n",
        "  with tensorflow.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)      # run the forward pass. training=True means it behaves in a training mode as said earlier\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)   # update the metrics's internal state/accumulators.\n",
        "    logs[metric.name] = metric.result()         # assigns the metric current value to the metric (name)\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "SMbTgQQrU4YS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to reset our metrics at the begining of each epoch and before the running evaluation. Thus ensures each epoch's metric value reflects the performance of only the epoch. lets see it below"
      ],
      "metadata": {
        "id": "A0MkNRJjRbj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "C_K0cXb_QDse"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets layout our complete training loop. notice we use tf.data.Dataset object, this turns our tensor data (a Numpy data) into iterator. It splits our data into individual (x,y) pairs that tensorflow can efficiently feed into our model. Lets write the complete training loop:"
      ],
      "metadata": {
        "id": "mRTcKDMN3DcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tensorflow.data.Dataset.from_tensor_slices((\n",
        "    train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)  # returned from the train_step function above. that is the metric, loss and their respective values\n",
        "  print(f\"results at the end of epoch {epoch}\")\n",
        "  for key, values in logs.items():\n",
        "    print(f\"... {key} : {values: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwnhVDC93APT",
        "outputId": "6aed597b-d174-4369-bcfa-1a93e3bcd16b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results at the end of epoch 0\n",
            "... sparse_categorical_accuracy :  0.9132\n",
            "... loss :  0.2906\n",
            "results at the end of epoch 1\n",
            "... sparse_categorical_accuracy :  0.9543\n",
            "... loss :  0.1597\n",
            "results at the end of epoch 2\n",
            "... sparse_categorical_accuracy :  0.9638\n",
            "... loss :  0.1301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is a training loops. we can also see that our model is learning well with the loss going down and the metric going up."
      ],
      "metadata": {
        "id": "H1_nJCgqAPMP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBboKIJWAisC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}