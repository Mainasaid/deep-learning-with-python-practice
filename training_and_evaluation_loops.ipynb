{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lKHQZVSQSdBf"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():                      # for the model to be reusable later\n",
        "  inputs = keras.Input((28 * 28,))\n",
        "  features = layers.Dense(512, activation='relu')(inputs)\n",
        "  features = layers.Dropout(0.5)(features)                 # randomly reducing the neurons to reduce overfitting.\n",
        "  outputs = layers.Dense(10, activation='softmax')(features)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yZgHc62Pceez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the mnist data\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "id": "sYa6bXK4cmEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a12622f-ac13-43f9-e0a4-8fce4826fb6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When writing a training loop, remember to pass \"training=True\" during the forward pass so that it behaves in a training mode (and Not inference mode)\n",
        "\n",
        "Also, to retrieve the gradients of the weights of the model, \"model.trainable_weights\" should be used, so that the weights will be updated via backpropagation to minize the loss of the model, such as in kernel, 'W' and bias, 'b'."
      ],
      "metadata": {
        "id": "jyWhP1ZDLjFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low level usage of metrics\n",
        "\n",
        "in the low-level training loop, you'll likely want to leverage the keras metrics.\n",
        "\n",
        "for the API: simply call update_state(y_true, y_pred) for each batch and predictions, and then use the result() argument to querry the current metric value. Lets see:"
      ],
      "metadata": {
        "id": "MrQeIMIBRX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result : {current_result:.2f}\")"
      ],
      "metadata": {
        "id": "Q1xBjSP0O2VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef50df96-2b5d-4175-c28d-4fbfb64a1ea1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, to track the average of a scaler like the model's loss, you can use the keras.mmetrics.Mean() metric as follows"
      ],
      "metadata": {
        "id": "_SS11zzkTr-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for val in values:\n",
        "  mean_tracker.update_state(val)\n",
        "print(f\"Mean of values : {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN-jvGpRTpsk",
        "outputId": "8afa1dd7-552e-4e1d-fa89-2a9a70bf63df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values : 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Remember to use metric.reset_state() when you want to reset the current results (at the start of a training epoch or at the start of evaluation). Just as the one used in the custom metric: \"RootMeanSquaredError()\" (check Testing.ipynb file for reference)"
      ],
      "metadata": {
        "id": "pEMg_HjAWPye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with all that we've seen, Lets now look at a complete training and evaluation loop.\n",
        "## Complete training and evaluation loop"
      ],
      "metadata": {
        "id": "kMqoLCLzW8df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the training loop function\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # prepares the list of metric to monitor. reports on how well is the model doin\n",
        "loss_tracking_metric = keras.metrics.Mean()     # keeps track of avrage of the losses\n",
        "\n",
        "def train_step(inputs, targets):                        # define one training iteration(one batch) using a custom loop instead of model.fit()\n",
        "  with tensorflow.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)      # run the forward pass. training=True means it behaves in a training mode as said earlier\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)   # update the metrics's internal state/accumulators.\n",
        "    logs[metric.name] = metric.result()         # assigns the metric current value to the metric (name)\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "SMbTgQQrU4YS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to reset our metrics at the begining of each epoch and before the running evaluation. Thus ensures each epoch's metric value reflects the performance of only the epoch. lets see it below"
      ],
      "metadata": {
        "id": "A0MkNRJjRbj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "C_K0cXb_QDse"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets layout our complete training loop. notice we use tf.data.Dataset object, this turns our tensor data (a Numpy data) into iterator. It splits our data into individual (x,y) pairs that tensorflow can efficiently feed into our model. Lets write the complete training loop:"
      ],
      "metadata": {
        "id": "mRTcKDMN3DcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tensorflow.data.Dataset.from_tensor_slices((\n",
        "    train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)  # returned from the train_step function above. that is the metric, loss and their respective values\n",
        "  print(f\"results at the end of epoch {epoch}\")\n",
        "  for key, values in logs.items():\n",
        "    print(f\"... {key} : {values: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwnhVDC93APT",
        "outputId": "fc65d809-7c37-4550-a210-052205916c4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results at the end of epoch 0\n",
            "... sparse_categorical_accuracy :  0.9133\n",
            "... loss :  0.2899\n",
            "results at the end of epoch 1\n",
            "... sparse_categorical_accuracy :  0.9547\n",
            "... loss :  0.1596\n",
            "results at the end of epoch 2\n",
            "... sparse_categorical_accuracy :  0.9633\n",
            "... loss :  0.1310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is a training loop. we can also see that our model is learning well — with the loss value going down and the metric going up."
      ],
      "metadata": {
        "id": "H1_nJCgqAPMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets look at the evaluation loop. its a simple for-loop that repeatedly calls the test_step() function, which processes a single batch of data. This functions is just a subset of the logic of train_step() function. That is to say it does not have codes that deals with updating the weights as in training : everything that involves Gradient_tape() and optimizer. lets see"
      ],
      "metadata": {
        "id": "H-BMvDioZ4Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(inputs, targets):\n",
        "  predictions = model(inputs, training=False)    # training=False here because it is for inference. not training, so no updating weights as you can see\n",
        "  loss = loss_fn(targets, predictions)\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)    # updates the metric internal state accumulator\n",
        "    logs[metric.name] = metric.result()          # assigns the metric current value to the metric\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)        # updates the average metric loss\n",
        "  logs['v_loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "yBboKIJWAisC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tensorflow.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs = test_step(inputs_batch, targets_batch)\n",
        "print('evaluation results:')\n",
        "for key, value in logs.items():\n",
        "  print(f'...{key}: {value:.4f}')"
      ],
      "metadata": {
        "id": "zZxZinNlIf4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ad8ca3-6ec8-4023-bf63-b8bef9dd7454"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation results:\n",
            "...sparse_categorical_accuracy: 0.9639\n",
            "...v_loss: 0.1287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have just implemented fit() and evaluation() method."
      ],
      "metadata": {
        "id": "XyG2xcGcgBUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "one noticeable thing is that these above custom loops, especially for fit runs significantly slower than the in-built fit(), and evaluation() method. we will take a look at tf.function optimizer next — to make it faster."
      ],
      "metadata": {
        "id": "TAkCJEK7gKBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the custom loops fast with tf.function\n",
        "The syntax to do this is very easy, just add @tf.function at the any function you want to compile before running it."
      ],
      "metadata": {
        "id": "dpd_RZQv32uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tNWZclNBfvIa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function                                      # Note that this is the only line that changed\n",
        "def train_step(inputs, targets):\n",
        "  with tensorflow.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[metric.name] = metric.result()\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs\n",
        "\n",
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "SegcJzDpCOtn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will run the training loop and compare the timing of execution with the first one without the @tf.function."
      ],
      "metadata": {
        "id": "73t5blY9Dq49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tensorflow.data.Dataset.from_tensor_slices((\n",
        "    train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)\n",
        "  print(f\"results at the end of epoch {epoch}\")\n",
        "  for key, values in logs.items():\n",
        "    print(f\"... {key} : {values: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egi8xfJIDX6B",
        "outputId": "b65d2cd1-5671-4e0c-98a1-14d2fe0e3cf3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results at the end of epoch 0\n",
            "... sparse_categorical_accuracy :  0.9681\n",
            "... loss :  0.1134\n",
            "results at the end of epoch 1\n",
            "... sparse_categorical_accuracy :  0.9719\n",
            "... loss :  0.1017\n",
            "results at the end of epoch 2\n",
            "... sparse_categorical_accuracy :  0.9738\n",
            "... loss :  0.0935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And yes, we just went from 2 minutes execution time to just 27 seconds using @tf.function on the train_step() function on google colab CPU."
      ],
      "metadata": {
        "id": "1VjqxGWnFkIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Remember that while debugging your code, its better you run it eagerly, i.e step by step, without the @tf.function.  once your code is okay, then you can add the @tf.function decorator to make it fast."
      ],
      "metadata": {
        "id": "2HoN0F_YOmKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom training step to use with fit()\n",
        "\n",
        "we can now leverage some features of fit(), which we missed building our training loop from scratch. features such as callbacks, or built-in support for distributed training.\n",
        "\n",
        "Now we will still have our custom training logic while simultaneously leveraging the power of the built-in keras training logic: we will provide custom training step function as done before, and let the framework do the rest.\n",
        "we will override the train_step() method of the Model class, then still be able to call the fit() as usual, and it will be ruinning our own learning algorithm under the hood.\n",
        "\n",
        "LETS SEE:"
      ],
      "metadata": {
        "id": "3-aqhck5zVMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name='loss')         # will be used to track the average of the per-batch losses during training & evaluation.\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "  def train_step(self, data):        # this is where we overide the model's train_step method\n",
        "    inputs, targets = data\n",
        "\n",
        "    with tensorflow.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)  # using self instead of model because the class itself is the model.\n",
        "      loss = loss_fn(targets, predictions)\n",
        "      gradients = tape.gradient(loss, self.trainable_weights)\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "\n",
        "    loss_tracker.update_state(loss)\n",
        "    return {'loss': loss_tracker.result()}        # we return the average loss so far by querrying the loss tracker metric\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [loss_tracker]       # any metric you would like to reset across epochs should be listed here. here it is the loss_tracker defined earlier\n"
      ],
      "metadata": {
        "id": "qncXP4WSOlNt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now instantiate our custom model. And because we have already defined our loss in the train_step, i.e. outside of the keras model, we only pass the the optimizer, and then use the fit() method as usual."
      ],
      "metadata": {
        "id": "ziWvaHw9nk8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation='relu')(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation='softmax')(features)\n",
        "model = CustomModel(inputs, outputs)                             # instantiating our model\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop())            # only the optimizer is passed. the loss has been defined outside the model\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va5z5VRXlpZv",
        "outputId": "6e609ad7-57cb-424c-9ba9-01ce33801c73"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - loss: 0.4369\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 0.1657\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - loss: 0.1291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f8cb585d3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the book, the weights in the train_step() method are wrritten as model.trainable weights and the optimizer as just optimizer.apply_gradients(). But this does'nt work because model and optimizer do not exist inside the class."
      ],
      "metadata": {
        "id": "dw6hLXpSzyYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to use the @tf.function decorator we used earler when you override the train_step() — the framework does it automatically.\n",
        "\n",
        "Also, the above pattern does not prevent us from building model Functional API models, sequential models, or subclassed models. customizing the training loop does not restrict how i build the model architecture."
      ],
      "metadata": {
        "id": "ZQZKOXsYhC6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now about metrics and configuring the loss via compile(): after calling compile(), you get access to the self.compiled_loss, self.compiled_meterics and self.metrics. lets see their usage below :"
      ],
      "metadata": {
        "id": "hxHuMHyjoRTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(keras.Model):\n",
        "  def train_step(self,data):\n",
        "    inputs, targets = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.compiled_loss(targets, predictions)  # compute the loss via self.compiled_loss\n",
        "    gradients = tape.gradient(loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "    self.compiled_metrics.update_state(targets, predictions) # update the model's metrics via self.compiled_metrics. updates all the metrics at once.\n",
        "    return {m.name: m.result() for m in self.metrics}   # returns a dict mapping the metric names with thier current values."
      ],
      "metadata": {
        "id": "uqKm8O6KyXWf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cju0hzYAtyHL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}